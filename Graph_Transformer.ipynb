{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f08a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic)\n",
      "  Using cached pydantic_core-2.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from pydantic) (4.12.2)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, annotated-types, pydantic\n",
      "Successfully installed annotated-types-0.7.0 pydantic-2.8.2 pydantic-core-2.20.1\n",
      "Collecting PyYAML\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Installing collected packages: PyYAML\n",
      "Successfully installed PyYAML-6.0.1\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Collecting torch==2.1.2\n",
      "  Using cached torch-2.1.2-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting torchvision==0.16.2\n",
      "  Using cached torchvision-0.16.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio==2.1.2\n",
      "  Using cached torchaudio-2.1.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from torch==2.1.2) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from torch==2.1.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from torch==2.1.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from torch==2.1.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from torch==2.1.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from torch==2.1.2) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from torchvision==0.16.2) (1.26.4)\n",
      "Collecting requests (from torchvision==0.16.2)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.16.2)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from jinja2->torch==2.1.2) (2.1.5)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchvision==0.16.2)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision==0.16.2)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchvision==0.16.2)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision==0.16.2)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/doductai/Desktop/AI and ML/Bresson/.venv/lib/python3.11/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Using cached torch-2.1.2-cp311-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "Using cached torchvision-0.16.2-cp311-cp311-macosx_11_0_arm64.whl (1.5 MB)\n",
      "Using cached torchaudio-2.1.2-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, pillow, idna, charset-normalizer, certifi, torch, requests, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 pillow-10.4.0 requests-2.32.3 torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2 urllib3-2.2.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install pydantic\n",
    "# !pip install PyYAML \n",
    "# !pip install numpy==1.26.4\n",
    "\n",
    "# !pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2\n",
    "\n",
    "# !pip install dgl -f https://data.dgl.ai/wheels/repo.html\n",
    "# !pip install ogb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nIXgomu-_nIF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIXgomu-_nIF",
    "outputId": "ddca6c0d-a57f-4936-d8c9-028e9212de1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doductai/Desktop/AI and ML/DGL projects/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.sparse as dglsp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# set dgl backend to pytorch\n",
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "\n",
    "from dgl.data import AsGraphPredDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from ogb.graphproppred import collate_dgl, DglGraphPropPredDataset, Evaluator\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t1jsZMko-GXU",
   "metadata": {
    "id": "t1jsZMko-GXU"
   },
   "source": [
    "## Architecture\n",
    "Original Paper: https://arxiv.org/pdf/2012.09699\n",
    "<img src=\"graph_tsfm.png\" width=\"300\" height='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e20de6",
   "metadata": {
    "id": "c0e20de6"
   },
   "source": [
    "## 1. Sparse attention\n",
    "\n",
    "### (a) Traditional Attention: $Q, K, V$\n",
    "1. Attention coefficients:\n",
    "$$\\text{attn\\_coeffs}(Q,K)=softmax\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right) \\ \\text{with} \\ d_k = \\text{dimensionality of} \\ Q, K$$\n",
    "2. Combine attention coefficients and values to give output\n",
    "$$\\text{attention}(Q,K,V)=\\text{attn\\_coeffs(Q,K)}V$$\n",
    "\n",
    "### (b) Sparse Attention\n",
    "Inputs: (1) hidden representation of node features $H = [N,\\text{hidden\\_dim}]$, (2) adjacency matrix $A$\n",
    "1. Project the input into query, key, value: Q, K, V = $[N,\\text{hidden\\_dim}]$\n",
    "2. Reshape Q,K,V according to number of heads $n_h$: $\\text{hidden\\_dim}=d_hn_h$ -> $Q,K,V=[N,d_h,n_h]$\n",
    "3. Compute sparse attention: * means pointwise multiplication\n",
    "$$\\text{attention}(Q,K,A)=\\text{softmax}\\left(\\dfrac{QK^T*A}{\\sqrt{d_h}}\\right),$$ \n",
    "    where the matrix $A$ controls the attention to be computed: $Q_iK_j^T$ is computed $\\Leftrightarrow A_{ij}=1 \\Leftrightarrow j\\in N(i)$.\n",
    "\n",
    "Remark: $K^T=K.\\text{tranpose}(1,0)=[d_h,N,n_h]$, $QK^T=[N,N,n_h]$, the pointwise multiplication $QK^T*A$ is applied on all $n_h$ heads.\n",
    "4. Combine with value $V$:\n",
    "$$\\text{SparseAttention(Q,K,V)}=\\text{attention}(Q,K,A)V $$\n",
    "with $\\text{attention}(Q,K,A)=[N,N,n_h], V=[N,d_h,n_h]$ and $\\text{SparseAttention(Q,K,V)}=[N,d_h,n_h]$\n",
    "5. (Optional) Reshape that output into $[N,\\text{hidden\\_dim}]$ and project it:\n",
    "$$\\text{SparseAttention(Q,K,V)}=W\\cdot\\text{SparseAttention(Q,K,V)}.reshape(N,-1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68cf5943",
   "metadata": {
    "id": "68cf5943"
   },
   "outputs": [],
   "source": [
    "# sparse attention module\n",
    "class SparseMHA(nn.Module):\n",
    "    def __init__(self,hidden_dim=80,num_heads=8):\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.num_heads=num_heads\n",
    "\n",
    "        self.linear_q=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.linear_k=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.linear_v=nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        # projection of the output\n",
    "        self.out_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "    def forward(self,A,h):\n",
    "        # A: [N,N], h: [N,hidden_dim]\n",
    "        N=len(h)\n",
    "        nh=self.num_heads\n",
    "        dh=self.hidden_dim//nh\n",
    "\n",
    "        # compute query,key, value\n",
    "        q=self.linear_q(h).reshape(N,dh,nh)\n",
    "        k=self.linear_k(h).reshape(N,dh,nh)\n",
    "        v=self.linear_k(h).reshape(N,dh,nh)\n",
    "\n",
    "        # compute attention scores by sparse matrix API: dglsp.bsddmm(A,X1,X2)\n",
    "        #                   compute (X1@X2)*A with X1,X2: dense matrices [N,dh,nh], [dh,N,nh]\n",
    "        #                   the pointwise multiplication applied along the last dim (batch dim = last dim)\n",
    "        attention_scores=dglsp.bsddmm(A,q,k.transpose(1,0)) # sparse [N,N,nh]\n",
    "\n",
    "        # sparse softmax: apply on the last dim by default\n",
    "        attention_scores=attention_scores.softmax()         # (sparse) [N,N,nh]\n",
    "\n",
    "        # apply value V: dglsp.bspmm(A,V) multiplies sparse matrix by dense matrix by batches\n",
    "        #                A=[N,N,nh], V=[N,dh,nh] -> output = [N,dh,nh]\n",
    "        out=dglsp.bspmm(attention_scores,v) # [N,dh,nh]\n",
    "\n",
    "        # concatentate the heads\n",
    "        out=out.reshape(N,-1) # [N,hidden_dim]\n",
    "\n",
    "        # project the output\n",
    "        return self.out_proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3641589",
   "metadata": {
    "id": "c3641589"
   },
   "source": [
    "## 2. Graph Transformer Layer\n",
    "<center>\n",
    "<img src=\"graph_tsfm.png\" width=\"300\" height='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11025f30",
   "metadata": {
    "id": "11025f30"
   },
   "outputs": [],
   "source": [
    "class GTLayer(nn.Module):\n",
    "    def __init__(self,hidden_dim=80,num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention=SparseMHA(hidden_dim,num_heads)\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.num_heads=num_heads\n",
    "\n",
    "        self.bn1=nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2=nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.ffn=nn.Sequential(nn.Linear(hidden_dim,2*hidden_dim),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(2*hidden_dim,hidden_dim))\n",
    "    def forward(self,A,h):\n",
    "        # A: [N,N], h: [N,hidden_dim]\n",
    "\n",
    "        # First add and norm\n",
    "        h1=self.attention(A,h) # [N,hidden_dim]\n",
    "        h=self.bn1(h+h1)\n",
    "\n",
    "        # Second add and norm\n",
    "        h2=self.ffn(h)\n",
    "        h=self.bn2(h+h2)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda01e6e",
   "metadata": {
    "id": "fda01e6e"
   },
   "source": [
    "## 3. Graph Transformer model\n",
    "Inputs -> GTLayers -> SumPooling -> Classifier\n",
    "\n",
    "1. SumPooling: extra pooler stacked on top of GT layers to aggregate node features of the same graph.\n",
    "2. Classifier = linear(d,d/2)+relu+linear(d/2,d/4)+relu+linear(d/4,out_size).\n",
    "\n",
    "Inputs: (1) Graph g, (2) Node features X, (3) pos_enc (Laplacian encoding): shape [N,k]\n",
    "\n",
    "First, we pass original data (dim d) into hidden_dim (that will be passed to the model):\n",
    "1. Project the nodes into hidden_dim: h=nn.Linear(d,hidden_dim)(x)\n",
    "2. Add position encoding to all nodes: h=h+pos_enc(h)\n",
    "\n",
    "Now iterate the output through layers:\n",
    "1. Compute adjaceny matrix A by g.edges(): indices=torch.stack(g.edges()), A=dglsp.spmatrix(indices,shape=(N,N))\n",
    "2. Pass h through the layers: h=self.layer(A,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420dd246",
   "metadata": {
    "id": "420dd246"
   },
   "outputs": [],
   "source": [
    "class GTModel(nn.Module):\n",
    "    def __init__(self, out_size,hidden_dim=80,num_heads=8,pos_enc_dim=2,num_layers=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # use atom encoder to project x into hidden representation h\n",
    "        self.atom_encoder=AtomEncoder(hidden_dim)\n",
    "\n",
    "        # map laplacian position encoding pos_enc into hidden_dim\n",
    "        self.pos_linear=nn.Linear(pos_enc_dim,hidden_dim)\n",
    "\n",
    "        # stack graph transformer layers\n",
    "        self.layers=nn.ModuleList(\n",
    "            [GTLayer(hidden_dim,num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # pooling layer\n",
    "        self.pooler=dglnn.SumPooling()\n",
    "\n",
    "        # classifier layer\n",
    "        d=hidden_dim\n",
    "        self.classifier=nn.Sequential(nn.Linear(d,d//2),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(d//2,d//4),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(d//4,out_size))\n",
    "\n",
    "    def forward(self,g,X,pos_enc):\n",
    "        # g = nx graph\n",
    "        indices=torch.stack(g.edges())\n",
    "        N=g.num_nodes()\n",
    "        A=dglsp.spmatrix(indices,shape=(N,N))\n",
    "        h=self.atom_encoder(X)+self.pos_linear(pos_enc)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h=layer(A,h)\n",
    "\n",
    "        # pooler aggregates node features of nodes in g\n",
    "        h=self.pooler(g,h)\n",
    "        # classify based on the aggregated nodes\n",
    "        h=self.classifier(h)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f608b",
   "metadata": {
    "id": "da0f608b"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "# out_size=1\n",
    "# model=GTModel(out_size=out_size,pos_enc_dim=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad694a8",
   "metadata": {
    "id": "6ad694a8"
   },
   "source": [
    "## 4. Dataset ogbg-molhiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c91764b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c91764b",
    "outputId": "6d804007-a361-4741-e79d-92d0181c267a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\"ogbg-molhiv-as-graphpred\", num_graphs=41127, save_path=/Users/doductai/.dgl/ogbg-molhiv-as-graphpred)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset as graph prediction data\n",
    "dataset = AsGraphPredDataset(\n",
    "    DglGraphPropPredDataset(\"ogbg-molhiv\", \"./data/OGB\")\n",
    ")\n",
    "evaluator = Evaluator(\"ogbg-molhiv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da738f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  tensor([0, 1])\n",
      "----- First Graph ------\n",
      "Number of nodes : 19 | Number of edges: 40 | Label: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "labels=torch.tensor([dataset[i][1] for i in range(len(dataset))])\n",
    "labels=labels.view(len(labels),)\n",
    "print(\"Labels: \", torch.unique(labels))\n",
    "\n",
    "g=dataset[0][0]\n",
    "label=dataset[0][1]\n",
    "print(\"----- First Graph ------\")\n",
    "print(f\"Number of nodes : {g.num_nodes()} | Number of edges: {g.num_edges()} | Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "600ad58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Train loader ---------\n",
      "Number of graphs: 32901 | Number of batches: 129\n",
      "--- First batch ---\n",
      "Graph(num_nodes=6510, num_edges=13934,\n",
      "      ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)})\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "# split data into train/validation/test\n",
    "train_loader=GraphDataLoader(dataset[dataset.train_idx],\n",
    "                             batch_size=batch_size,shuffle=True,collate_fn=collate_dgl)\n",
    "val_loader=GraphDataLoader(dataset[dataset.val_idx],\n",
    "                             batch_size=batch_size,shuffle=False,collate_fn=collate_dgl)\n",
    "test_loader=GraphDataLoader(dataset[dataset.test_idx],\n",
    "                             batch_size=batch_size,shuffle=False,collate_fn=collate_dgl)\n",
    "print(\"--------- Train loader ---------\")\n",
    "print(f\"Number of graphs: {len(train_loader.dataset)} | Number of batches: {len(train_loader)}\")\n",
    "print(\"--- First batch ---\")\n",
    "for batch,labels in train_loader:\n",
    "    print(batch)\n",
    "    print(labels[:20].view(20,))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419c89b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "419c89b6",
    "outputId": "57c0bde4-11e5-472c-ded6-fbe4925e9047"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Laplacian PE: 100%|██████████| 41127/41127 [01:09<00:00, 593.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# laplacian positional encoding\n",
    "pos_enc_dim=2*batch.ndata['feat'].shape[-1]\n",
    "indices=torch.cat([dataset.train_idx,dataset.val_idx,dataset.test_idx])\n",
    "for idx in tqdm(indices, desc=\"Computing Laplacian PE\"):\n",
    "    g,_=dataset[idx]\n",
    "    g.ndata[\"PE\"]=dgl.lap_pe(g,k=pos_enc_dim,padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t_ZIR63Jvkd2",
   "metadata": {
    "id": "t_ZIR63Jvkd2"
   },
   "source": [
    "## 5. Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2d49cfe",
   "metadata": {
    "id": "e2d49cfe"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model,loader,loss_fn,optimizer,device):\n",
    "    total_loss=0\n",
    "    model.train()\n",
    "    for batch,labels in loader:\n",
    "        batch,labels=batch.to(device),labels.to(device)\n",
    "\n",
    "        out_logits=model(batch,batch.ndata[\"feat\"],batch.ndata[\"PE\"])\n",
    "        loss=loss_fn(out_logits,labels.float())\n",
    "        total_loss+=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss/len(loader)\n",
    "\n",
    "def evaluation(model,loader,evaluator,device):\n",
    "    model.eval()\n",
    "    y_true, y_pred=[],[]\n",
    "    for batch,labels in loader:\n",
    "        batch,labels=batch.to(device), labels.to(device)\n",
    "        y_hat=model(batch,batch.ndata[\"feat\"],batch.ndata[\"PE\"])\n",
    "\n",
    "        y_true.append(labels.view(y_hat.shape,).detach().cpu())\n",
    "        y_pred.append(y_hat.detach().cpu())\n",
    "\n",
    "    y_true=torch.cat(y_true,dim=0).numpy()\n",
    "    y_pred=torch.cat(y_pred,dim=0).numpy()\n",
    "\n",
    "    input_dict={\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70898570",
   "metadata": {
    "id": "70898570"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def train_and_test(model,train_loader,val_loader,test_loader,num_epochs,loss_fn,evaluator,optimizer,device):\n",
    "\n",
    "    best_model=None\n",
    "    best_val_acc=0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss=train(model,train_loader,loss_fn,optimizer,device)\n",
    "\n",
    "        train_result=evaluation(model,train_loader,evaluator,device)\n",
    "        val_result=evaluation(model,val_loader,evaluator,device)\n",
    "        test_result=evaluation(model,test_loader,evaluator,device)\n",
    "\n",
    "        train_acc,val_acc,test_acc=train_result['rocauc'], val_result['rocauc'], test_result['rocauc']\n",
    "\n",
    "        # save the best model\n",
    "        if val_acc>best_val_acc:\n",
    "            best_val_acc=val_acc\n",
    "            best_model=copy.deepcopy(model)\n",
    "\n",
    "        print(f'Epoch: {epoch} | Train loss: {train_loss:.4f} | train_roc: {train_acc*100:.2f}% | '\n",
    "         f'val_roc: {val_acc*100:.2f}% | test_roc: {test_acc*100:.2f}%')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aca27b5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aca27b5e",
    "outputId": "709375d7-a7a9-49fa-fe4a-e3da4f7cb7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.1811 | train_roc: 68.10% | val_roc: 64.28% | test_roc: 55.74%\n",
      "Epoch: 1 | Train loss: 0.1485 | train_roc: 78.95% | val_roc: 67.25% | test_roc: 67.43%\n",
      "Epoch: 2 | Train loss: 0.1349 | train_roc: 80.52% | val_roc: 66.73% | test_roc: 69.19%\n",
      "Epoch: 3 | Train loss: 0.1299 | train_roc: 78.26% | val_roc: 62.48% | test_roc: 71.47%\n",
      "Epoch: 4 | Train loss: 0.1197 | train_roc: 87.78% | val_roc: 64.13% | test_roc: 67.45%\n",
      "Epoch: 5 | Train loss: 0.1101 | train_roc: 92.43% | val_roc: 77.60% | test_roc: 73.67%\n",
      "Epoch: 6 | Train loss: 0.1046 | train_roc: 92.67% | val_roc: 76.53% | test_roc: 72.93%\n",
      "Epoch: 7 | Train loss: 0.1019 | train_roc: 93.68% | val_roc: 75.16% | test_roc: 71.64%\n",
      "Epoch: 8 | Train loss: 0.0912 | train_roc: 95.07% | val_roc: 76.04% | test_roc: 73.37%\n",
      "Epoch: 9 | Train loss: 0.0888 | train_roc: 97.28% | val_roc: 78.46% | test_roc: 71.54%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create model\n",
    "model=GTModel(out_size=1,pos_enc_dim=pos_enc_dim).to(device)\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "# BCEWithLogitsLoss() = sigmoid + BCE (more stable than plain BCE applied on sigmoid)\n",
    "loss_fn=torch.nn.BCEWithLogitsLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "best_model=train_and_test(model,train_loader,val_loader,test_loader,num_epochs,loss_fn,evaluator,optimizer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18651da3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18651da3",
    "outputId": "b8ecec12-b024-47bc-9d84-23cf5f9eb5e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model ROCAUC on test set: 0.7154\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on test set\n",
    "best_model_acc=evaluation(best_model,test_loader,evaluator,device)['rocauc']\n",
    "print(f\"Best model ROCAUC on test set: {best_model_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KgSUDc8uptsn",
   "metadata": {
    "id": "KgSUDc8uptsn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
